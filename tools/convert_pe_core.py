#!/usr/bin/env python3
"""
PE-Core-L14-336 ëª¨ë¸ì„ TFLiteë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

í•„ìš”í•œ íŒ¨í‚¤ì§€:
pip install torch torchvision transformers huggingface_hub onnx tf2onnx tensorflow

ì‚¬ìš©ë²•:
python tools/convert_pe_core.py
"""

import os
import sys
import torch
import torch.nn as nn
import numpy as np
from pathlib import Path

# HuggingFace í† í° ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
HF_TOKEN = os.getenv("HF_TOKEN")
if not HF_TOKEN:
    print("âš ï¸  ê²½ê³ : HF_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("HuggingFace í† í°ì´ í•„ìš”í•œ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í•˜ì„¸ìš”:")
    print("export HF_TOKEN=your_token_here")


def download_pe_core_model():
    """PE-Core-L14-336 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    print("ğŸ“¦ PE-Core ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")

    try:
        # perception_models ê²½ë¡œ ì¶”ê°€
        perception_models_path = Path(__file__).parent / "perception_models"

        if not perception_models_path.exists():
            print("âŒ perception_models ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("\në‹¤ìŒ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”:")
            print("1. cd tools")
            print(
                "2. git clone https://github.com/facebookresearch/perception_models.git"
            )
            print("3. cd perception_models")
            print("4. pip install -e .")
            sys.exit(1)

        sys.path.insert(0, str(perception_models_path))

        # PE-Core ëª¨ë¸ ë¡œë“œ
        import core.vision_encoder.pe as pe

        print("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì²˜ìŒ ì‹¤í–‰ ì‹œ 2.7GB ë‹¤ìš´ë¡œë“œ)")
        model = pe.CLIP.from_config("PE-Core-L14-336", pretrained=True)
        model.eval()

        # ëª¨ë¸ ì†ì„± í™•ì¸ (ë””ë²„ê¹…)
        print("\n=== PE-Core ëª¨ë¸ ì†ì„± ===")
        print("ëª¨ë¸ ì†ì„±:", [attr for attr in dir(model) if not attr.startswith("_")])
        print("ì£¼ìš” ì†ì„±:")
        for attr in [
            "visual",
            "text",
            "transformer",
            "token_embedding",
            "encode_text",
            "encode_image",
        ]:
            if hasattr(model, attr):
                print(f"  âœ… {attr}: {type(getattr(model, attr))}")
            else:
                print(f"  âŒ {attr}: ì—†ìŒ")
        print("========================\n")

        print("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        return model
    except ImportError as e:
        print(f"âŒ perception_models ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        print("\nperception_models ì„¤ì¹˜:")
        print("cd tools/perception_models && pip install -e .")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        sys.exit(1)


def export_vision_encoder_to_onnx(model, output_path):
    """Vision Encoderë¥¼ ONNXë¡œ ë³€í™˜"""
    print("\nğŸ”„ Vision Encoderë¥¼ ONNXë¡œ ë³€í™˜ ì¤‘...")

    # Vision encoder ì¶”ì¶œ (PE-CoreëŠ” visual ì†ì„± ì‚¬ìš©)
    vision_encoder = model.visual
    vision_encoder.eval()

    # ë”ë¯¸ ì…ë ¥ (336x336x3, CHW í¬ë§·)
    dummy_input = torch.randn(1, 3, 336, 336)

    # ONNX ë‚´ë³´ë‚´ê¸°
    torch.onnx.export(
        vision_encoder,
        dummy_input,
        output_path,
        export_params=True,
        opset_version=18,
        do_constant_folding=True,
        input_names=["image"],
        output_names=["vision_embedding"],  # ê³ ìœ í•œ ì´ë¦„
        dynamic_axes={
            "image": {0: "batch_size"},
            "vision_embedding": {0: "batch_size"},
        },
    )

    print(f"âœ… Vision Encoder ONNX ì €ì¥ ì™„ë£Œ: {output_path}")


def export_text_encoder_to_onnx(model, output_path):
    """Text Encoderë¥¼ ONNXë¡œ ë³€í™˜"""
    print("\nğŸ”„ Text Encoderë¥¼ ONNXë¡œ ë³€í™˜ ì¤‘...")

    # PE-Coreì˜ context_length í™•ì¸
    context_length = getattr(model, "context_length", 32)
    print(f"  í…ìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {context_length}")

    # Text encodingì„ ìœ„í•œ wrapper ìƒì„±
    class TextEncoderWrapper(torch.nn.Module):
        def __init__(self, clip_model):
            super().__init__()
            self.token_embedding = clip_model.token_embedding
            self.positional_embedding = clip_model.positional_embedding
            self.transformer = clip_model.transformer
            self.ln_final = clip_model.ln_final
            self.text_projection = clip_model.text_projection

        def forward(self, text):
            # PE-Coreì˜ encode_text ë¡œì§ ì¬êµ¬í˜„
            x = self.token_embedding(text)
            x = x + self.positional_embedding[: text.shape[1]]
            x = x.permute(1, 0, 2)  # NLD -> LND
            x = self.transformer(x)
            x = x.permute(1, 0, 2)  # LND -> NLD
            x = self.ln_final(x)

            # í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ì¶œ (EOS í† í° ìœ„ì¹˜)
            x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection
            return x

    text_wrapper = TextEncoderWrapper(model)
    text_wrapper.eval()

    # ë”ë¯¸ ì…ë ¥ (ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë§ì¶¤)
    dummy_input = torch.randint(0, model.vocab_size, (1, context_length))

    print("  í…ìŠ¤íŠ¸ ì¸ì½”ë” wrapper ìƒì„± ì™„ë£Œ")

    # ONNX ë‚´ë³´ë‚´ê¸°
    torch.onnx.export(
        text_wrapper,
        dummy_input,
        output_path,
        export_params=True,
        opset_version=18,
        do_constant_folding=True,
        input_names=["tokens"],
        output_names=["text_embedding"],  # ê³ ìœ í•œ ì´ë¦„
        dynamic_axes={"tokens": {0: "batch_size"}, "text_embedding": {0: "batch_size"}},
    )

    print(f"âœ… Text Encoder ONNX ì €ì¥ ì™„ë£Œ: {output_path}")


def consolidate_onnx_model(onnx_path, quantize_fp16=True):
    """ì™¸ë¶€ ë°ì´í„° íŒŒì¼ì„ ONNX ëª¨ë¸ì— í¬í•¨í•˜ê³  float16ìœ¼ë¡œ ì–‘ìí™”"""
    import onnx
    from onnx import numpy_helper
    from pathlib import Path
    import numpy as np

    print(f"  ì²˜ë¦¬ ì¤‘: {onnx_path}")

    # ONNX ëª¨ë¸ ë¡œë“œ (ì™¸ë¶€ ë°ì´í„° í¬í•¨)
    model = onnx.load(onnx_path, load_external_data=True)

    # Float16 ì–‘ìí™”
    if quantize_fp16:
        print(f"  ğŸ”„ Float16 ì–‘ìí™” ì¤‘...")
        from onnxconverter_common import float16

        model = float16.convert_float_to_float16(
            model,
            keep_io_types=True,  # ì…ì¶œë ¥ì€ float32 ìœ ì§€
        )
        print(f"  âœ… Float16 ì–‘ìí™” ì™„ë£Œ (í¬ê¸° ~50% ê°ì†Œ)")

    # ì™¸ë¶€ ë°ì´í„°ë¥¼ ë‚´ë¶€ë¡œ í†µí•©í•˜ì—¬ ì €ì¥
    onnx.save(
        model,
        onnx_path,
        save_as_external_data=False,  # ì™¸ë¶€ ë°ì´í„° íŒŒì¼ ì‚¬ìš© ì•ˆ í•¨
    )

    # ì™¸ë¶€ ë°ì´í„° íŒŒì¼ ì‚­ì œ
    data_file = Path(onnx_path).with_suffix(".onnx.data")
    if data_file.exists():
        data_file.unlink()

    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size_mb = Path(onnx_path).stat().st_size / (1024 * 1024)
    print(f"  âœ… ìµœì¢… íŒŒì¼ í¬ê¸°: {file_size_mb:.1f} MB")


def convert_onnx_to_tflite(onnx_path, tflite_path, quantize=True):
    """ONNX ëª¨ë¸ì„ TFLiteë¡œ ë³€í™˜"""
    print(f"\nğŸ”„ ONNX â†’ TFLite ë³€í™˜ ì¤‘: {onnx_path}")

    try:
        import onnx
        import tensorflow as tf
        from onnx_tf.backend import prepare

        # ONNX ëª¨ë¸ ë¡œë“œ
        onnx_model = onnx.load(onnx_path)

        # TensorFlowë¡œ ë³€í™˜
        tf_rep = prepare(onnx_model)

        # SavedModelë¡œ ì €ì¥
        temp_dir = str(Path(tflite_path).parent / "temp_saved_model")
        tf_rep.export_graph(temp_dir)

        # TFLiteë¡œ ë³€í™˜
        converter = tf.lite.TFLiteConverter.from_saved_model(temp_dir)

        if quantize:
            # Float16 ì–‘ìí™”
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            converter.target_spec.supported_types = [tf.float16]
            print("  ğŸ“‰ Float16 ì–‘ìí™” ì ìš©")

        tflite_model = converter.convert()

        # TFLite ëª¨ë¸ ì €ì¥
        with open(tflite_path, "wb") as f:
            f.write(tflite_model)

        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ
        import shutil

        shutil.rmtree(temp_dir, ignore_errors=True)

        # íŒŒì¼ í¬ê¸° í™•ì¸
        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)
        print(f"âœ… TFLite ë³€í™˜ ì™„ë£Œ: {tflite_path} ({size_mb:.2f} MB)")

    except ImportError as e:
        print(f"âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("pip install onnx onnx-tf tensorflow")
        sys.exit(1)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("PE-Core-L14-336 TFLite ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 60)

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(__file__).parent.parent / "assets" / "models"
    output_dir.mkdir(parents=True, exist_ok=True)

    temp_dir = Path(__file__).parent / "temp"
    temp_dir.mkdir(exist_ok=True)

    try:
        # 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        model = download_pe_core_model()

        # 2. Vision Encoder â†’ ONNX
        vision_onnx_path = temp_dir / "pe_core_vision_l14.onnx"
        export_vision_encoder_to_onnx(model, str(vision_onnx_path))

        # 3. Text Encoder â†’ ONNX
        text_onnx_path = temp_dir / "pe_core_text_l14.onnx"
        export_text_encoder_to_onnx(model, str(text_onnx_path))

        # 4. ì™¸ë¶€ ë°ì´í„° íŒŒì¼ì„ ëª¨ë¸ì— í¬í•¨ (ì–‘ìí™” ë¹„í™œì„±í™”)
        print("\nğŸ”„ ì™¸ë¶€ ë°ì´í„°ë¥¼ ONNX íŒŒì¼ì— í†µí•© ì¤‘...")
        consolidate_onnx_model(str(vision_onnx_path), quantize_fp16=False)
        consolidate_onnx_model(str(text_onnx_path), quantize_fp16=False)

        print("\n" + "=" * 60)
        print("âœ… ONNX ë³€í™˜ ì™„ë£Œ!")
        print("=" * 60)
        print(f"Vision Encoder: {vision_onnx_path}")
        print(f"Text Encoder: {text_onnx_path}")

        # TFLite ë³€í™˜ì€ ê±´ë„ˆë›°ê³  ONNX Runtime ì‚¬ìš©
        print("\nâš ï¸  TFLite ë³€í™˜ì€ onnx-tf í˜¸í™˜ì„± ë¬¸ì œë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")
        print("ëŒ€ì‹  ONNX Runtime for Flutterë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        print("\nğŸ“ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. ONNX íŒŒì¼ì„ assets/models/ë¡œ ë³µì‚¬:")
        print(f"   cp {temp_dir}/pe_core_vision_l14.onnx ../assets/models/")
        print(f"   cp {temp_dir}/pe_core_text_l14.onnx ../assets/models/")
        print("2. Flutter í”„ë¡œì íŠ¸ì—ì„œ 'flutter pub get' ì‹¤í–‰")
        print("3. ì•±ì„ ë¹Œë“œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)
    # finally ë¸”ë¡ ì œê±° - ONNX íŒŒì¼ì„ ë³´ì¡´í•˜ê¸° ìœ„í•´


if __name__ == "__main__":
    main()
